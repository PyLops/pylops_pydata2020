{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Authors: M. Ravasi, D. Vargas, I. Vasconcelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the **Solving large-scale inverse problems in Python with PyLops** tutorial!\n",
    "\n",
    "The aim of this tutorial is to:\n",
    "\n",
    "- introduce you to the concept of *linear operators* and their usage in the solution of *inverse problems*;\n",
    "- show how PyLops can be used to set-up non-trivial linear operators and solve inverse problems in Python; \n",
    "- Walk you through a set of use cases where PyLops has been leveraged to solve real scientific problems and present future directions of development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "\n",
    "- Tutorial Github repository: https://github.com/mrava87/pylops_pydata2020\n",
    "        \n",
    "- PyLops Github repository: https://github.com/equinor/pylops\n",
    "\n",
    "- PyLops reference documentation: https://pylops.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory in a nutshell\n",
    "\n",
    "In this tutorial we will try to keep the theory to a minimum and quickly expose you to practical examples. However, we want to make sure that some of the basic underlying concepts are clear to everyone and define a common mathematical notation.\n",
    "\n",
    "At the core of PyLops lies the concept of **linear operators**. A linear operator is generally a mapping or function that acts linearly on elements of a space to produce elements of another space. More specifically we say that $\\mathbf{A}:\\mathbb{F}^m \\to \\mathbb{F}^n$ is a linear operator that maps a vector of size $m$ in the *model space* to a vector of size $n$ in the *data space*:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{A} \\mathbf{x}$$\n",
    "\n",
    "We will refer to this as **forward model (or operation)**. \n",
    "\n",
    "Conversely the application of its adjoint to a data vector is referred to as **adjoint modelling (or operation)**:\n",
    "\n",
    "$$\\mathbf{x} = \\mathbf{A}^H \\mathbf{y}$$\n",
    "\n",
    "In its simplest form, a linear operator can be seen as a **matrix** of size $n \\times m$ (and the adjoint is simply its transpose and complex conjugate). However in a more general sense we can think of a linear operator as any pair of software code that mimics the effect a matrix on a model vector as well as that of its adjoint to a data vector.\n",
    "\n",
    "Solving an inverse problems accounts to removing the effect of the operator/matrix $\\mathbf{A}$ from the data $\\mathbf{y}$ to retrieve the model $\\mathbf{x}$ (or an approximation of it).\n",
    "\n",
    "$$\\hat{\\mathbf{x}} = \\mathbf{A}^{-1} \\mathbf{y}$$\n",
    "\n",
    "In practice, the inverse of $\\mathbf{A}$ is generally not explicitely required. A solution can be obtained using either direct methods, matrix decompositions (eg SVD) or iterative solvers. Luckily, many iterative methods (e.g. cg, lsqr) do not need to know the individual entries of a matrix to solve a linear system. Such solvers only require the computation of forward and adjoint matrix-vector products - exactly what a linear operator does!\n",
    "\n",
    "**So what?**\n",
    "We have learned that to solve an inverse problem, we do not need to express the modelling operator in terms of its dense (or sparse) matrix. All we need to know is how to perform the forward and adjoint operations - ideally as fast as possible and using the least amount of memory. \n",
    "\n",
    "Our first task will be to understand how we can effectively write a linear operator on pen and paper and translate it into computer code. We will consider 2 examples:\n",
    "\n",
    "- Element-wise multiplication (also known as Hadamard product)\n",
    "- First Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the libraries we need in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylops\n",
    "import scooby\n",
    "\n",
    "from scipy.linalg import lstsq\n",
    "from pylops import LinearOperator\n",
    "from pylops.utils import dottest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a barebore linear operator that performs a simple element-wise multiplication between two vectors (the so-called Hadamart product):\n",
    "\n",
    "$$ y_i = d_i x_i  \\quad \\forall i=0,1,...,n-1 $$\n",
    "\n",
    "If we think about the forward problem the way we wrote it before, we can see that this operator can be equivalently expressed as a dot-product between a square matrix $\\mathbf{D}$ that has the $d_i$ elements along its main diagonal and a vector $\\mathbf{x}$:\n",
    "\n",
    "$$\\mathbf{x} = \\mathbf{D} \\mathbf{y}$$\n",
    "\n",
    "Because of this, the related linear operator is called *Diagonal* operator in PyLops.\n",
    "\n",
    "We are ready to implement this operator in 2 different ways:\n",
    "\n",
    "- directly as a diagonal matrix; \n",
    "- as a linear operator that performs directly element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense matrix definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "diag = np.arange(n)\n",
    "\n",
    "D = np.diag(diag)\n",
    "print('D:\\n', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the forward by simply using `np.dot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(n)\n",
    "y = np.dot(D, x) # or D.dot(x) or D @ x\n",
    "print('y: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have access to all the entries of the matrix, it is very easy to write the adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xadj = np.dot(np.conj(D.T), y)\n",
    "print('xadj: ', xadj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* since the elements of our matrix are real numbers, we can avoid applying the complex conjugation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems very easy so far. This approach does however carry some problems:\n",
    "    \n",
    "- we are storing $N^2$ numbers, even though we know that our matrix has only elements along its diagonal.\n",
    "- we are applying a dot product which requires $N^2$ multiplications and summations (most of them with zeros)\n",
    "\n",
    "Of course in this case we could use a sparse matrix, which allows to store only non-zero elements (and their index) and provides a faster way to perform the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operator definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a leap of faith, and see if we can avoid thinking about the matrix altogether and write just an equivalent (ideally faster) piece of code that mimics this operation.\n",
    "\n",
    "To write its equivalent linear operator, we define a class with an init method, and 2 other methods:\n",
    "    \n",
    "- _matvec: we write the forward operation here\n",
    "- _rmatvec: we write the adjoint operation here\n",
    "    \n",
    "We see that we are also subclassing a PyLops LinearOperator. For the moment let's not get into the details of what that entails and simply focus on writing the content of these three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diagonal(LinearOperator):\n",
    "    \"\"\"Short version of a Diagonal operator. See\n",
    "    https://github.com/equinor/pylops/blob/master/pylops/basicoperators/Diagonal.py\n",
    "    for a more detailed implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, diag, dtype='float64'):\n",
    "        self.diag = diag\n",
    "        self.shape = (len(self.diag), len(self.diag))\n",
    "        self.dtype = np.dtype(dtype)\n",
    "\n",
    "    def _matvec(self, x):\n",
    "        y = self.diag * x\n",
    "        return y\n",
    "\n",
    "    def _rmatvec(self, x):\n",
    "        y = np.conj(self.diag) * x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dop = Diagonal(diag)\n",
    "print('Dop: ', Dop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operator application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Dop * x # Dop @ x\n",
    "print('y: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xadj = Dop.H * y\n",
    "print('xadj: ', xadj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we obtain the same results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EX:** try making a much bigger vector $\\mathbf{x}$ and time the forward and adjoint for the two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diagonal_timing():\n",
    "    \"\"\"Timing of Diagonal operator\n",
    "    \"\"\"\n",
    "    n = 10000\n",
    "    diag = np.arange(n)\n",
    "    x = np.ones(n)\n",
    "\n",
    "    # dense\n",
    "    D = np.diag(diag)\n",
    "\n",
    "    from scipy import sparse\n",
    "    Ds = sparse.diags(diag, 0)\n",
    "\n",
    "    # lop\n",
    "    Dop = Diagonal(diag)\n",
    "\n",
    "    # uncomment these\n",
    "    %timeit -n3 -r3 np.dot(D, x)\n",
    "    %timeit -n3 -r3 Ds.dot(x)\n",
    "    %timeit -n3 -r3 Dop._matvec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diagonal_timing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operator testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important aspect of writing a Linear operator is to be able to verify that the code implemented in forward mode and the code implemented in adjoint mode are effectively adjoint to each other. \n",
    "\n",
    "If this is not the case, we will struggle to invert our linear operator - some iterative solvers will diverge and other show very slow convergence.\n",
    "\n",
    "This is instead the case if the so-called *dot-test* is passed within a certain treshold:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}*\\mathbf{u})^H*\\mathbf{v} = \\mathbf{u}^H*(\\mathbf{A}^H*\\mathbf{v})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{u}$ and $\\mathbf{v}$ are two random vectors.\n",
    "\n",
    "Let's use `pylops.utils.dottest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dottest(Dop, n, n, verb=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now something less trivial. We use a first-order centered first derivative stencil:\n",
    "\n",
    "$$ y_i = \\frac{x_{i+1} - x_{i-1}}{2 \\Delta}  \\quad \\forall i=1,2,...,N $$\n",
    "\n",
    "where $\\Delta$ is the sampling step of the input signal. Note that we will deal differently with the edges, using a forward/backward derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense matrix definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 11\n",
    "\n",
    "D = np.diag(0.5*np.ones(nx-1), k=1) - np.diag(0.5*np.ones(nx-1), k=-1) \n",
    "D[0, 0] = D[-1, -2] = -1\n",
    "D[0, 1] = D[-1, -1] = 1\n",
    "print('D:\\n', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operator definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EX:** try writing the operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FirstDerivative(LinearOperator):\n",
    "    def __init__(self, ...):\n",
    "        # fill here\n",
    "\n",
    "    def _matvec(self, x):\n",
    "        # fill here\n",
    "\n",
    "    def _rmatvec(self, x):\n",
    "        # fill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstDerivative(LinearOperator):\n",
    "    \"\"\"Short version of a FirstDerivative operator. See\n",
    "    https://github.com/equinor/pylops/blob/master/pylops/basicoperators/FirstDerivative.py\n",
    "    for a more detailed implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, N, sampling=1., dtype='float64'):\n",
    "        self.N = N\n",
    "        self.sampling = sampling\n",
    "        self.shape = (N, N)\n",
    "        self.dtype = dtype\n",
    "        self.explicit = False\n",
    "\n",
    "    def _matvec(self, x):\n",
    "        x, y = x.squeeze(), np.zeros(self.N, self.dtype)\n",
    "        y[1:-1] = (0.5 * x[2:] - 0.5 * x[0:-2]) / self.sampling\n",
    "        # edges\n",
    "        y[0] = (x[1] - x[0]) / self.sampling\n",
    "        y[-1] = (x[-1] - x[-2]) / self.sampling\n",
    "        return y\n",
    "\n",
    "    def _rmatvec(self, x):\n",
    "        x, y = x.squeeze(), np.zeros(self.N, self.dtype)\n",
    "        y[0:-2] -= (0.5 * x[1:-1]) / self.sampling\n",
    "        y[2:] += (0.5 * x[1:-1]) / self.sampling\n",
    "        # edges\n",
    "        y[0] -= x[0] / self.sampling\n",
    "        y[1] += x[0] / self.sampling\n",
    "        y[-2] -= x[-1] / self.sampling\n",
    "        y[-1] += x[-1] / self.sampling\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dop = FirstDerivative(nx)\n",
    "print('Dop: ', Dop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the dot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dottest(Dop, nx, nx, verb=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand, you can use PyLops implementation of this operator (see https://pylops.readthedocs.io/en/latest/api/generated/pylops.FirstDerivative.html for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dop = pylops.FirstDerivative(nx, edge=True)\n",
    "print('Dop: ', Dop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dottest(Dop, nx, nx, verb=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operator application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(nx) - (nx-1)/2\n",
    "print('x: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.dot(D, x)\n",
    "print('y: ', y)\n",
    "\n",
    "y = Dop * x\n",
    "print('y: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xadj = np.dot(D.T, y)\n",
    "print('xadj: ', xadj)\n",
    "\n",
    "xadj = Dop.H * y\n",
    "print('xadj: ', xadj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EX:** Same as before, let's time our two implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstDerivative_timing():\n",
    "    \"\"\"Timing of FirstDerivative operator\n",
    "    \"\"\"\n",
    "    nx = 2001\n",
    "    x = np.arange(nx) - (nx-1)/2\n",
    "\n",
    "    # dense\n",
    "    D = np.diag(0.5*np.ones(nx-1),k=1) - np.diag(0.5*np.ones(nx-1),-1)\n",
    "    D[0, 0] = D[-1, -2] = -1\n",
    "    D[0, 1] = D[-1, -1] = 1\n",
    "\n",
    "    # lop\n",
    "    Dop = pylops.FirstDerivative(nx, edge=True)\n",
    "\n",
    "    # uncomment these\n",
    "    %timeit -n3 -r3 np.dot(D, x)\n",
    "    %timeit -n3 -r3 Dop._matvec(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstDerivative_timing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EX:** try to compare the memory footprint of the matrix $\\mathbf{D}$ compared to its equivalent linear operator. Hint: install ``pympler`` and use ``pympler.asizeof``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstDerivative_memory():\n",
    "    \"\"\"Memory footprint of Diagonal operator\n",
    "    \"\"\"\n",
    "    from pympler import asizeof\n",
    "    from scipy.sparse import diags\n",
    "    nn = (10 ** np.arange(2, 4, 0.5)).astype(np.int)\n",
    "\n",
    "    mem_D = []\n",
    "    mem_Ds = []\n",
    "    mem_Dop = []\n",
    "    for n in nn:\n",
    "        D = np.diag(0.5 * np.ones(n - 1), k=1) - np.diag(0.5 * np.ones(n - 1),\n",
    "                                                         -1)\n",
    "        D[0, 0] = D[-1, -2] = -1\n",
    "        D[0, 1] = D[-1, -1] = 1\n",
    "        Ds = diags((0.5 * np.ones(n - 1), -0.5 * np.ones(n - 1)),\n",
    "                   offsets=(1, -1))\n",
    "        Dop = pylops.FirstDerivative(n, edge=True)\n",
    "        mem_D.append(asizeof.asizeof(D))\n",
    "        mem_Ds.append(asizeof.asizeof(Ds))\n",
    "        mem_Dop.append(asizeof.asizeof(Dop))\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.semilogy(nn, mem_D, '.-k', label='D')\n",
    "    plt.semilogy(nn, mem_Ds, '.-b', label='Ds')\n",
    "    plt.semilogy(nn, mem_Dop, '.-r', label='Dop')\n",
    "    plt.legend()\n",
    "    plt.title('Memory comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstDerivative_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to move on step further and try to solve the inverse problem. \n",
    "\n",
    "For the dense matrix, we will use `scipy.linalg.lstsq`. For operator PyLops this can be very easily done by using the '/' which will call `scipy.sparse.linalg.lsqr` solver (you can also use this solver directly if you want to fine tune some of its input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinv = lstsq(D, y)[0]\n",
    "print('xinv: ', xinv)\n",
    "\n",
    "xinv = Dop / y\n",
    "print('xinv: ', xinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we have retrieved the correct solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining operators\n",
    "\n",
    "Up until now, we have discussed how brand new operators can be created in few systematic steps. This sounds cool, but it may look like we would need to do this every time we need to solve a new problem.\n",
    "\n",
    "This is where **PyLops** comes in. Alongside providing users with an extensive collection of operators, the library allows such operators to be combined via basic algebraic operations (eg summed, subtracted, multiplied) or chained together (vertical and horizontal stacking, block and block diagonal).\n",
    "\n",
    "We will see more of this in the following. For now let's imagine to have a modelling operator that is a smooth first-order derivative. To do so we can chain the ``FirstDerivative`` operator ($\\mathbf{D}$) that we have just created with a smoothing operator ($\\mathbf{S}$)(https://pylops.readthedocs.io/en/latest/api/generated/pylops.Smoothing1D.html#pylops.Smoothing1D) and write the following problem:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{S} \\mathbf{D} \\mathbf{x}$$\n",
    "\n",
    "Let's create it first and attempt to invert it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 51\n",
    "x = np.ones(nx)\n",
    "x[:nx//2] = -1\n",
    "\n",
    "Dop = pylops.FirstDerivative(nx, edge=True, kind='forward')\n",
    "Sop = pylops.Smoothing1D(5, nx)\n",
    "\n",
    "# Chain the two operators\n",
    "Op = Sop * Dop\n",
    "print(Op)\n",
    "\n",
    "# Create data\n",
    "y = Op * x\n",
    "\n",
    "# Invert\n",
    "xinv = Op / y\n",
    "xinv = pylops.optimization.leastsquares.NormalEquationsInversion(Op, [pylops.Identity(nx)], y, epsRs=[1e-3,])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 5))\n",
    "ax1.plot(y, '.-k')\n",
    "ax1.set_title(r\"Data $y$\")\n",
    "ax2.plot(x, 'k', label='x')\n",
    "ax2.plot(xinv, '--r', label='xinv')\n",
    "ax2.legend()\n",
    "ax2.set_title(r\"Model $x$\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this first tutorial we have learned to:\n",
    "\n",
    "- translate a linear operator from pen and paper to computer code\n",
    "- write our own linear operators\n",
    "- use PyLops linear operators to perform forward, adjoint and inverse\n",
    "- combine PyLops linear operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby.Report(core='pylops')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
